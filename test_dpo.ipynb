{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.2 in ./venv/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: tensorboard in ./venv/lib/python3.10/site-packages (2.16.2)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.10/site-packages (10.3.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (11.4.5.107)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (3.3)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (4.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (8.9.2.26)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (3.1.4)\n",
      "Requirement already satisfied: triton==2.1.0 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (12.1.105)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch==2.1.2) (3.14.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2) (12.5.40)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./venv/lib/python3.10/site-packages (from tensorboard) (1.64.0)\n",
      "Requirement already satisfied: six>1.9 in ./venv/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./venv/lib/python3.10/site-packages (from tensorboard) (65.5.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./venv/lib/python3.10/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.10/site-packages (from tensorboard) (3.0.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./venv/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.10/site-packages (from tensorboard) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in ./venv/lib/python3.10/site-packages (from tensorboard) (5.27.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch==2.1.2) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers[sentencepiece]==4.37.2 in ./venv/lib/python3.10/site-packages (4.37.2)\n",
      "Requirement already satisfied: datasets==2.16.1 in ./venv/lib/python3.10/site-packages (2.16.1)\n",
      "Requirement already satisfied: accelerate==0.26.1 in ./venv/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: evaluate==0.4.1 in ./venv/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: bitsandbytes==0.42.0 in ./venv/lib/python3.10/site-packages (0.42.0)\n",
      "Requirement already satisfied: trl==0.7.11 in ./venv/lib/python3.10/site-packages (0.7.11)\n",
      "Requirement already satisfied: peft==0.8.2 in ./venv/lib/python3.10/site-packages (0.8.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (6.0.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (3.14.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (0.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (2024.5.15)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (24.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (0.23.1)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (4.66.4)\n",
      "Requirement already satisfied: protobuf in ./venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (5.27.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in ./venv/lib/python3.10/site-packages (from transformers[sentencepiece]==4.37.2) (0.2.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.10/site-packages (from datasets==2.16.1) (2.2.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./venv/lib/python3.10/site-packages (from datasets==2.16.1) (0.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./venv/lib/python3.10/site-packages (from datasets==2.16.1) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.10/site-packages (from datasets==2.16.1) (3.9.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./venv/lib/python3.10/site-packages (from datasets==2.16.1) (0.3.7)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in ./venv/lib/python3.10/site-packages (from datasets==2.16.1) (2023.10.0)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.10/site-packages (from datasets==2.16.1) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.10/site-packages (from datasets==2.16.1) (0.70.15)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./venv/lib/python3.10/site-packages (from accelerate==0.26.1) (2.1.2)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.10/site-packages (from accelerate==0.26.1) (5.9.8)\n",
      "Requirement already satisfied: responses<0.19 in ./venv/lib/python3.10/site-packages (from evaluate==0.4.1) (0.18.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.10/site-packages (from bitsandbytes==0.42.0) (1.13.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in ./venv/lib/python3.10/site-packages (from trl==0.7.11) (0.8.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.9.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (6.0.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[sentencepiece]==4.37.2) (4.12.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.37.2) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.37.2) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.37.2) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests->transformers[sentencepiece]==4.37.2) (3.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (8.9.2.26)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.1.3.1)\n",
      "Requirement already satisfied: triton==2.1.0 in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (2.1.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.26.1) (12.5.40)\n",
      "Requirement already satisfied: rich>=11.1.0 in ./venv/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11) (13.7.1)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in ./venv/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./venv/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.11) (1.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.10/site-packages (from pandas->datasets==2.16.1) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.26.1) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.11) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Pytorch & other libraries\n",
    "!pip install \"torch==2.1.2\" tensorboard pillow\n",
    " \n",
    "# Install Hugging Face libraries\n",
    "!pip install  --upgrade \\\n",
    "  \"transformers[sentencepiece]==4.37.2\" \\\n",
    "  \"datasets==2.16.1\" \\\n",
    "  \"accelerate==0.26.1\" \\\n",
    "  \"evaluate==0.4.1\" \\\n",
    "  \"bitsandbytes==0.42.0\" \\\n",
    "  \"trl==0.7.11\" \\\n",
    "  \"peft==0.8.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in ./venv/lib/python3.10/site-packages (1.11.1.1)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (24.0)\n",
      "Requirement already satisfied: wheel in ./venv/lib/python3.10/site-packages (0.43.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: flash-attn in ./venv/lib/python3.10/site-packages (2.5.8)\n",
      "Requirement already satisfied: einops in ./venv/lib/python3.10/site-packages (from flash-attn) (0.8.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.10/site-packages (from flash-attn) (24.0)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.10/site-packages (from flash-attn) (2.1.2)\n",
      "Requirement already satisfied: ninja in ./venv/lib/python3.10/site-packages (from flash-attn) (1.11.1.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (3.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (2023.10.0)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (11.0.2.54)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (4.12.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.0.106)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (3.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (11.4.5.107)\n",
      "Requirement already satisfied: triton==2.1.0 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.10/site-packages (from torch->flash-attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "# install flash-attn\n",
    "!pip install ninja packaging wheel\n",
    "!MAX_JOBS=4 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d8e8506dfe4a75a108fe232e05f418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    " \n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toskov/project-code-2024/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9767b8cecf54ed494da4a4493a4e5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Dolphin, a helpful AI a\n",
      "<|im_start|>assistant\n",
      "Developing games for a VR he\n",
      "<|im_start|>assistant\n",
      "Hello! I'd be happy to help!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de753fc945684c68af3b1580cc8fd167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cdd495eb844004b777abdc3d301e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9569503"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    " \n",
    "# Load Tokenizer from the hub\n",
    "model_id = \"cognitivecomputations/dolphin-2.1-mistral-7b\" # replace with your model id\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    " \n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(13750))\n",
    " \n",
    " \n",
    "def rec_extract_assistant_messages(messages, index=-1):\n",
    "  \"\"\"Recursively extract the last assistant messages from the end of the conversation.\"\"\"\n",
    "  if messages[index][\"role\"] == \"assistant\":\n",
    "    return [messages[index]]\n",
    "  else:\n",
    "    return rec_extract_assistant_messages(messages, index-1)\n",
    " \n",
    "# System message used if there is no system message at the beginning of the conversation\n",
    "# Can be repelaced and modified as needed\n",
    "DEFAULT_SYSTEM_MESSAGE = \"You are Dolphin, a helpful AI assistant.\"\n",
    " \n",
    "def create_triplets(example, tokenizer, default_system_message=DEFAULT_SYSTEM_MESSAGE):\n",
    "  \"\"\"Create the triplets (prompt, chosen, rejected)\"\"\"\n",
    "  # Extract the N-1 turns to form the prompt\n",
    "  # Prepend a system message if the first message is not a system message\n",
    "  prompt_messages = example[\"chosen\"][:-1]\n",
    "  if example[\"chosen\"][0][\"role\"] != \"system\":\n",
    "      prompt_messages.insert(0, {\"role\": \"system\", \"content\": default_system_message})\n",
    "  # Now we extract the final assistant turn to define chosen/rejected responses\n",
    "  chosen_messages = rec_extract_assistant_messages(example[\"chosen\"])\n",
    "  rejected_messages = rec_extract_assistant_messages(example[\"rejected\"])\n",
    " \n",
    "  # apply template to the messages and return the triplets\n",
    "  return {\n",
    "    \"prompt\": tokenizer.apply_chat_template(prompt_messages, tokenize=False),\n",
    "    \"chosen\": tokenizer.apply_chat_template(chosen_messages, tokenize=False),\n",
    "    \"rejected\": tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "  }\n",
    " \n",
    "dataset = dataset.map(create_triplets, remove_columns=dataset.features, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "# split dataset into 11,000 training samples and 2,750 test samples\n",
    "dataset = dataset.train_test_split(test_size=2750/13750)\n",
    " \n",
    "# print sample cut of\n",
    "print(dataset[\"train\"][0][\"prompt\"][:50])\n",
    "print(dataset[\"train\"][0][\"chosen\"][:50])\n",
    "print(dataset[\"train\"][0][\"rejected\"][:50])\n",
    " \n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98401a60af05481c9e636f8154330b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b639b9a25a2d48f98d8290bbce4f6db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    " \n",
    "# Load jsonl data from disk\n",
    "train_dataset = load_dataset(\"json\", data_files=\"train_dataset.json\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toskov/project-code-2024/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23305c948bc4e25b1c0b7eb5f0eb342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toskov/project-code-2024/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    " \n",
    "# Hugging Face model id\n",
    "model_id = \"cognitivecomputations/dolphin-2.1-mistral-7b\" # replace with your model id\n",
    " \n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    " \n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' # to prevent errors with FA\n",
    "tokenizer.truncation_side = 'left' # to prevent cutting off last generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMMENT IN TO RECALCULATE MAX LENGTHS ####\n",
    "# from numpy import percentile\n",
    " \n",
    "# # lets find the p95 length of the prompt\n",
    "# prompt_length = int(percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"prompt\"]], 95))\n",
    "# max_seq_length_chosen = int(percentile([len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
    "# max_seq_length_rejected = int(percentile([len(tokenizer(x[\"prompt\"] + x[\"rejected\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
    "# max_seq_length = max(max_seq_length_chosen, max_seq_length_rejected)\n",
    " \n",
    "# # filter datasets to remove samples that are too long\n",
    "# train_dataset = train_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
    "# eval_dataset = eval_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
    "# print(f\"len(train_dataset): {len(train_dataset)}\")\n",
    "# print(f\"len(eval_dataset): {len(eval_dataset)}\")\n",
    " \n",
    "# # Up the lengths to next multiple of 2, why 2? Don't know\n",
    "# prompt_length = ((prompt_length + 1) // 2) * 2\n",
    "# max_seq_length = ((max_seq_length + 1) // 2) * 2\n",
    "# print(f\"p95 prompt length: {prompt_length}\")\n",
    "# print(f\"p95 prompt + chosen length: {max_seq_length}\")\n",
    " \n",
    "prompt_length = 1024\n",
    "max_seq_length = 1512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    " \n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    " \n",
    "args = TrainingArguments(\n",
    "    output_dir=\"doplhin-dpo\",               # directory to save and repository id\n",
    "    num_train_epochs=1,                     # number of training epochs\n",
    "    per_device_train_batch_size=5,         # batch size per device during training\n",
    "    per_device_eval_batch_size=4,           # batch size for evaluation\n",
    "    gradient_accumulation_steps=1,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    learning_rate=5e-5,                     # 10x higher LR than QLoRA paper\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.1,                       # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"cosine\",             # use cosine learning rate scheduler\n",
    "    logging_steps=25,                       # log every 25 steps\n",
    "    save_steps=500,                         # when to save checkpoint\n",
    "    save_total_limit=2,                     # limit the total amount of checkpoints\n",
    "    evaluation_strategy=\"steps\",            # evaluate every 1000 steps\n",
    "    eval_steps=700,                         # when to evaluate\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    push_to_hub=False,                      # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")\n",
    " \n",
    "dpo_args = {\n",
    "    \"beta\": 0.1,                            # The beta factor in DPO loss. Higher beta means less divergence\n",
    "    \"loss_type\": \"sigmoid\"                  # The loss type for DPO.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toskov/project-code-2024/venv/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:328: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6655628864914a2e842124d93c62f3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64508b795062483caf15dc85568993d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import DPOTrainer\n",
    " \n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None, # set to none since we use peft\n",
    "    peft_config=peft_config,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_seq_length,\n",
    "    max_prompt_length=prompt_length,\n",
    "    beta=dpo_args[\"beta\"],\n",
    "    loss_type=dpo_args[\"loss_type\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toskov/project-code-2024/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "/home/toskov/project-code-2024/venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='141' max='2200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 141/2200 13:03 < 3:13:25, 0.18 it/s, Epoch 0.06/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n",
    " \n",
    "# save model at the end of training\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free the memory again\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMMENT IN TO MERGE PEFT AND BASE MODEL ####\n",
    "# from peft import PeftModel, PeftConfig\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from peft import AutoPeftModelForCausalLM\n",
    " \n",
    "# # Load PEFT model on CPU\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     args.output_dir,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     low_cpu_mem_usage=True,\n",
    "# )\n",
    "# # Merge LoRA and base model and save\n",
    "# merged_model = model.merge_and_unload()\n",
    "# merged_model.save_pretrained(args.output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    " \n",
    "# Path to saved peft adapter model\n",
    "# peft_model_id = args.output_dir # or\n",
    "peft_model_id = \"./doplhin-dpo\"\n",
    " \n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "  \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "  \"It's Bengay for muscle relief, a combination of methyl salicylate, menthol, and what other active ingredient commonly found in aspirin?\",\n",
    "  \"How can i get rid of llamas in my backyard?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "  messages = pipe.tokenizer.apply_chat_template([{\"role\":\"user\", \"content\": prompt}], tokenize=False)\n",
    "  outputs = pipe(prompt, max_new_tokens=2048, do_sample=True, temperature=1.0, top_k=50, top_p=0.9, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)\n",
    "  print(f\"**Prompt**:\\n{prompt}\\n\")\n",
    "  print(f\"**Generated Answer**:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")\n",
    "  print(\"===\" * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
